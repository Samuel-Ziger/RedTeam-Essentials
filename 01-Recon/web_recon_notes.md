# ğŸŒ Web Reconnaissance - Notas Completas

> Guia detalhado sobre reconhecimento de aplicaÃ§Ãµes web de forma passiva e ativa

---

## ğŸ“‹ Ãndice

1. [IntroduÃ§Ã£o ao Web Recon](#introduÃ§Ã£o)
2. [Footprinting Passivo](#footprinting-passivo)
3. [IdentificaÃ§Ã£o de Tecnologias](#identificaÃ§Ã£o-de-tecnologias)
4. [EnumeraÃ§Ã£o de DiretÃ³rios e Arquivos](#enumeraÃ§Ã£o)
5. [AnÃ¡lise de AplicaÃ§Ã£o](#anÃ¡lise-de-aplicaÃ§Ã£o)
6. [Web Crawling](#web-crawling)
7. [Ferramentas Essenciais](#ferramentas)
8. [Metodologia Completa](#metodologia)

---

## ğŸ¯ IntroduÃ§Ã£o

**Web Reconnaissance** Ã© o processo de coletar informaÃ§Ãµes sobre uma aplicaÃ§Ã£o web para entender sua estrutura, tecnologias, possÃ­veis pontos de entrada e superfÃ­cie de ataque.

### Objetivos do Web Recon
- ğŸ” Descobrir tecnologias utilizadas
- ğŸ“‚ Mapear estrutura de diretÃ³rios
- ğŸ” Identificar mecanismos de autenticaÃ§Ã£o
- ğŸšª Encontrar possÃ­veis pontos de entrada
- ğŸ“ Coletar informaÃ§Ãµes sobre a aplicaÃ§Ã£o

---

## ğŸ•µï¸ Footprinting Passivo

### 1. AnÃ¡lise Inicial da URL

```
https://www.exemplo.com:443/login.php?redirect=/dashboard

Breakdown:
â”œâ”€ Protocolo: https (SSL/TLS)
â”œâ”€ SubdomÃ­nio: www
â”œâ”€ DomÃ­nio: exemplo.com
â”œâ”€ Porta: 443 (padrÃ£o HTTPS)
â”œâ”€ Recurso: /login.php
â””â”€ ParÃ¢metros: ?redirect=/dashboard
```

**O que observar:**
- Uso de HTTPS vs HTTP
- Estrutura de subdomÃ­nios
- ExtensÃµes de arquivo (.php, .aspx, .jsp)
- ParÃ¢metros GET sensÃ­veis

---

### 2. Robots.txt

O arquivo `robots.txt` indica aos crawlers quais Ã¡reas **nÃ£o** devem ser indexadas.

```
Acesso: https://exemplo.com/robots.txt
```

**Exemplo de robots.txt:**
```
User-agent: *
Disallow: /admin/
Disallow: /backup/
Disallow: /config/
Disallow: /private/
Allow: /public/

Sitemap: https://exemplo.com/sitemap.xml
```

**Por que Ã© importante:**
- Revela diretÃ³rios sensÃ­veis
- Mostra Ã¡reas que o site quer esconder
- Pode conter caminhos interessantes

âš ï¸ **ATENÃ‡ÃƒO**: Robots.txt NÃƒO Ã© medida de seguranÃ§a! Ã‰ apenas uma sugestÃ£o para crawlers.

---

### 3. Sitemap.xml

Lista estruturada de todas as pÃ¡ginas do site.

```
Acesso: https://exemplo.com/sitemap.xml
```

**Exemplo:**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <url>
    <loc>https://exemplo.com/</loc>
    <lastmod>2025-01-15</lastmod>
  </url>
  <url>
    <loc>https://exemplo.com/produtos/</loc>
    <lastmod>2025-01-10</lastmod>
  </url>
  <url>
    <loc>https://exemplo.com/admin/dashboard/</loc>
    <lastmod>2025-01-05</lastmod>
  </url>
</urlset>
```

**InformaÃ§Ãµes Ãºteis:**
- Estrutura completa do site
- PÃ¡ginas ocultas nÃ£o linkadas
- Datas de modificaÃ§Ã£o

---

### 4. AnÃ¡lise de CabeÃ§alhos HTTP

```bash
# Usando curl
curl -I https://exemplo.com

# Resultado esperado:
HTTP/2 200
server: nginx/1.18.0
date: Fri, 22 Nov 2025 10:00:00 GMT
content-type: text/html; charset=UTF-8
x-powered-by: PHP/7.4.3
set-cookie: PHPSESSID=abc123; HttpOnly; Secure
strict-transport-security: max-age=31536000
x-frame-options: SAMEORIGIN
x-content-type-options: nosniff
```

**O que analisar:**

| CabeÃ§alho | InformaÃ§Ã£o Revelada |
|-----------|---------------------|
| `Server` | Servidor web (nginx, Apache, IIS) |
| `X-Powered-By` | Linguagem/Framework (PHP, ASP.NET) |
| `Set-Cookie` | Flags de seguranÃ§a (HttpOnly, Secure) |
| `X-Frame-Options` | ProteÃ§Ã£o contra Clickjacking |
| `Content-Security-Policy` | PolÃ­ticas de seguranÃ§a |

---

## ğŸ”§ IdentificaÃ§Ã£o de Tecnologias

### Ferramentas Automatizadas

#### 1. Wappalyzer (ExtensÃ£o Browser)
- Detecta CMS (WordPress, Joomla, Drupal)
- Frameworks (React, Angular, Vue.js)
- Servidores web
- Analytics e tracking

#### 2. BuiltWith
```
https://builtwith.com/exemplo.com
```
Mostra:
- Stack tecnolÃ³gico completo
- ServiÃ§os de hospedagem
- CDNs utilizadas
- Plugins e widgets

#### 3. WhatWeb (CLI)
```bash
whatweb https://exemplo.com

# SaÃ­da exemplo:
https://exemplo.com [200 OK] 
  Country: UNITED STATES
  HTTPServer: nginx/1.18.0
  IP: 93.184.216.34
  PoweredBy: PHP/7.4.3
  Title: Exemplo Site
  WordPress: 5.8
```

---

### DetecÃ§Ã£o Manual

#### Assinaturas no HTML
```html
<!-- WordPress -->
<meta name="generator" content="WordPress 5.8" />

<!-- Joomla -->
<meta name="generator" content="Joomla! - Open Source Content Management" />

<!-- Drupal -->
<meta name="Generator" content="Drupal 9" />
```

#### Estrutura de DiretÃ³rios PadrÃ£o
```
WordPress:
â”œâ”€ /wp-admin/
â”œâ”€ /wp-content/
â”œâ”€ /wp-includes/
â””â”€ /wp-login.php

Joomla:
â”œâ”€ /administrator/
â”œâ”€ /components/
â”œâ”€ /modules/
â””â”€ /templates/

Drupal:
â”œâ”€ /core/
â”œâ”€ /modules/
â”œâ”€ /sites/
â””â”€ /themes/
```

---

## ğŸ“‚ EnumeraÃ§Ã£o

### DiretÃ³rios e Arquivos Comuns

```
Arquivos de ConfiguraÃ§Ã£o:
â”œâ”€ .env
â”œâ”€ config.php
â”œâ”€ settings.py
â”œâ”€ web.config
â””â”€ application.yml

Arquivos SensÃ­veis:
â”œâ”€ .git/
â”œâ”€ .svn/
â”œâ”€ backup.sql
â”œâ”€ database.sql
â””â”€ phpinfo.php

PainÃ©is Admin:
â”œâ”€ /admin/
â”œâ”€ /administrator/
â”œâ”€ /manager/
â”œâ”€ /cpanel/
â””â”€ /dashboard/
```

### Ferramentas de Directory Bruteforce

âš ï¸ **ATENÃ‡ÃƒO**: Use apenas em ambientes autorizados!

#### Gobuster
```bash
gobuster dir -u https://exemplo.com -w /path/to/wordlist.txt

OpÃ§Ãµes Ãºteis:
-t 50          # 50 threads
-x php,html    # Buscar extensÃµes especÃ­ficas
-s 200,301     # Filtrar por status code
-o output.txt  # Salvar resultados
```

#### Dirsearch
```bash
dirsearch -u https://exemplo.com -e php,html,js

OpÃ§Ãµes:
-t 50          # Threads
-r             # Recursivo
-x 403,404     # Excluir status codes
```

#### Feroxbuster (RÃ¡pido e Recursivo)
```bash
feroxbuster -u https://exemplo.com -w wordlist.txt -t 100 -d 2

-d 2           # Profundidade de recursÃ£o
-t 100         # Threads
```

---

### Wordlists Recomendadas

```
SecLists (GitHub):
â”œâ”€ Discovery/Web-Content/
â”‚  â”œâ”€ common.txt
â”‚  â”œâ”€ big.txt
â”‚  â”œâ”€ directory-list-2.3-medium.txt
â”‚  â””â”€ raft-large-directories.txt
â”‚
â””â”€ Discovery/DNS/
   â”œâ”€ subdomains-top1million.txt
   â””â”€ dns-Jhaddix.txt
```

---

## ğŸ” AnÃ¡lise de AplicaÃ§Ã£o

### 1. FormulÃ¡rios e Inputs

```html
<form action="/login.php" method="POST">
    <input type="text" name="username" />
    <input type="password" name="password" />
    <input type="hidden" name="csrf_token" value="abc123" />
    <button type="submit">Login</button>
</form>
```

**O que observar:**
- âœ… MÃ©todo (GET vs POST)
- âœ… PresenÃ§a de CSRF tokens
- âœ… ValidaÃ§Ã£o client-side (JavaScript)
- âœ… Campos hidden
- âœ… Autocomplete habilitado

---

### 2. JavaScript e APIs

#### Encontrar Endpoints de API
```javascript
// Procurar no cÃ³digo-fonte por:
fetch('/api/users')
axios.get('/api/data')
$.ajax({url: '/api/search'})

// PadrÃµes comuns:
/api/v1/
/rest/
/graphql
/api/internal/
```

#### Ferramentas para AnÃ¡lise JS
- **JSParser** - Extrai endpoints de arquivos JS
- **LinkFinder** - Encontra URLs em JavaScript
- **Browser DevTools** - Aba Network para ver requests

```bash
# LinkFinder
python linkfinder.py -i https://exemplo.com -o cli
```

---

### 3. Cookies e SessÃµes

```
AnÃ¡lise de Cookie:
PHPSESSID=abc123def456; Path=/; HttpOnly; Secure; SameSite=Strict

Flags de SeguranÃ§a:
â”œâ”€ HttpOnly â†’ Previne acesso via JavaScript (XSS)
â”œâ”€ Secure â†’ SÃ³ envia via HTTPS
â”œâ”€ SameSite â†’ Previne CSRF
â””â”€ Path â†’ Escopo do cookie
```

**Ferramenta:** Browser Developer Tools â†’ Application â†’ Cookies

---

## ğŸ•·ï¸ Web Crawling

### Crawlers Automatizados

#### Burp Suite Spider
1. Configure o proxy (127.0.0.1:8080)
2. Navegue pelo site manualmente
3. Target â†’ Site Map
4. Right-click â†’ Spider this host

#### OWASP ZAP Spider
```
1. Quick Start â†’ Automated Scan
2. Ou: Tools â†’ Spider â†’ Add URL
```

#### Scrapy (Python)
```python
import scrapy

class ExemploSpider(scrapy.Spider):
    name = 'exemplo'
    start_urls = ['https://exemplo.com']
    
    def parse(self, response):
        # Extrai todos os links
        for link in response.css('a::attr(href)').getall():
            yield {'url': link}
        
        # Segue links
        for href in response.css('a::attr(href)').getall():
            yield response.follow(href, self.parse)
```

---

### Crawling Manual

**Ãreas importantes:**
```
â”œâ”€ PÃ¡ginas de Login/Registro
â”œâ”€ FormulÃ¡rios de contato
â”œâ”€ Upload de arquivos
â”œâ”€ Perfis de usuÃ¡rio
â”œâ”€ Painel administrativo
â”œâ”€ API endpoints
â”œâ”€ DocumentaÃ§Ã£o
â””â”€ PÃ¡ginas de erro (404, 500)
```

---

## ğŸ› ï¸ Ferramentas Essenciais

### Suite Completa - Burp Suite
```
Recursos principais:
â”œâ”€ Proxy â†’ Interceptar requests
â”œâ”€ Spider â†’ Crawler automÃ¡tico
â”œâ”€ Repeater â†’ Replay requests
â”œâ”€ Intruder â†’ Fuzzing e brute force
â”œâ”€ Scanner â†’ Detectar vulnerabilidades (Pro)
â””â”€ Extensions â†’ Expandir funcionalidades
```

### Alternativa Open Source - OWASP ZAP
```
Funcionalidades:
â”œâ”€ Active Scanner
â”œâ”€ Passive Scanner
â”œâ”€ Spider
â”œâ”€ Fuzzer
â”œâ”€ API Support
â””â”€ Add-ons
```

### Linha de Comando

| Ferramenta | Uso |
|------------|-----|
| **curl** | Requests HTTP manuais |
| **wget** | Download recursivo |
| **httpie** | HTTP client user-friendly |
| **nuclei** | Scanner de vulnerabilidades |
| **ffuf** | Fuzzing rÃ¡pido |

---

## ğŸ“‹ Metodologia Completa

### Fase 1: Reconhecimento Passivo
```
1. [ ] Acessar site e navegar manualmente
2. [ ] Verificar robots.txt
3. [ ] Analisar sitemap.xml
4. [ ] Inspecionar cÃ³digo-fonte HTML
5. [ ] Analisar cabeÃ§alhos HTTP
6. [ ] Identificar tecnologias (Wappalyzer)
7. [ ] Buscar no Wayback Machine
8. [ ] Google Dorking do site
```

### Fase 2: Mapeamento Ativo
```
1. [ ] Spider/Crawler automÃ¡tico
2. [ ] EnumeraÃ§Ã£o de diretÃ³rios
3. [ ] Busca de arquivos sensÃ­veis
4. [ ] Identificar parÃ¢metros GET/POST
5. [ ] Mapear formulÃ¡rios
6. [ ] Extrair endpoints de APIs
7. [ ] Analisar JavaScript
8. [ ] Testar pÃ¡ginas de erro
```

### Fase 3: AnÃ¡lise Profunda
```
1. [ ] Testar autenticaÃ§Ã£o
2. [ ] Verificar HTTPS/TLS
3. [ ] Analisar cookies e sessÃµes
4. [ ] Buscar comentÃ¡rios no cÃ³digo
5. [ ] Verificar versionamento (.git, .svn)
6. [ ] Testar upload de arquivos
7. [ ] Verificar CORS
8. [ ] Analisar CSP headers
```

### Fase 4: DocumentaÃ§Ã£o
```
1. [ ] Criar mapa visual do site
2. [ ] Listar todas as URLs encontradas
3. [ ] Documentar tecnologias
4. [ ] Anotar possÃ­veis vetores de ataque
5. [ ] Organizar screenshots
6. [ ] Preparar relatÃ³rio
```

---

## ğŸ¯ Checklist de Web Recon

### InformaÃ§Ãµes BÃ¡sicas
- [ ] URL principal e subdomÃ­nios
- [ ] Servidor web (nginx, Apache, IIS)
- [ ] Linguagem backend (PHP, Python, .NET)
- [ ] Framework (Laravel, Django, Express)
- [ ] CMS (WordPress, Drupal, Joomla)
- [ ] Frontend framework (React, Vue, Angular)

### Arquivos SensÃ­veis
- [ ] robots.txt
- [ ] sitemap.xml
- [ ] .git/ ou .svn/
- [ ] backup files (.bak, .old, ~)
- [ ] config files (.env, config.php)
- [ ] phpinfo.php
- [ ] README, CHANGELOG

### SeguranÃ§a
- [ ] HTTPS configurado corretamente
- [ ] CabeÃ§alhos de seguranÃ§a presentes
- [ ] Cookies com flags corretas
- [ ] CSRF tokens implementados
- [ ] Rate limiting em formulÃ¡rios
- [ ] Captcha em Ã¡reas sensÃ­veis

---

## ğŸ’¡ Dicas de Ouro

### 1. Combine Ferramentas
NÃ£o dependa de uma Ãºnica ferramenta:
```
Wappalyzer + WhatWeb + BuiltWith = VisÃ£o completa das tecnologias
Gobuster + Dirsearch + Feroxbuster = EnumeraÃ§Ã£o abrangente
```

### 2. Use Wordlists Contextuais
```
Site em PHP? Use wordlist focada em PHP
WordPress? Use wordlist de plugins/themes do WP
API? Use wordlist de endpoints REST
```

### 3. Preste AtenÃ§Ã£o aos Detalhes
```
404 personalizado pode revelar framework
Mensagens de erro expÃµem paths internos
ComentÃ¡rios HTML podem ter TODOs dos devs
```

### 4. Documente TUDO
```
Tire screenshots
Salve responses completas
Anote comportamentos estranhos
Mantenha log de timestamps
```

---

## âš ï¸ Avisos Legais

### Reconhecimento Ativo vs Passivo

**Passivo** (Geralmente Legal):
- Acessar site normalmente
- Ver cÃ³digo-fonte HTML
- Consultar robots.txt, sitemap
- Usar ferramentas pÃºblicas (Wappalyzer)

**Ativo** (Requer AutorizaÃ§Ã£o):
- Directory bruteforce
- Fuzzing de parÃ¢metros
- Testes de vulnerabilidade
- Scanning automatizado

### Regra de Ouro
> **Sempre tenha autorizaÃ§Ã£o por escrito antes de fazer testes ativos!**

---

## ğŸ“š Recursos para Estudo

### LaboratÃ³rios PrÃ¡ticos
- **PortSwigger Web Security Academy** - Labs gratuitos
- **HackTheBox** - MÃ¡quinas web reais
- **TryHackMe** - Rooms de web hacking
- **PentesterLab** - ExercÃ­cios focados

### Leituras Recomendadas
- OWASP Testing Guide
- Web Application Hacker's Handbook
- Real-World Bug Hunting

### YouTube Channels
- NahamSec
- STÃ–K
- InsiderPhD
- PwnFunction

---

<div align="center">

**ğŸŒ Web Recon Ã© a fundaÃ§Ã£o de todo web pentest**

*Conhecer a aplicaÃ§Ã£o Ã© o primeiro passo para protegÃª-la*

---

*ConteÃºdo educacional | Use responsavelmente*

</div>
